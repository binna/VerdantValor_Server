# Verdant Valor Server Project (2025.10.18 ~ 진행 중)

서버 프로젝트입니다.   
웹 서버, 게임 서버, 채팅 서버를 구축하는 것을 목표로 만들고 있습니다.   

▼ 이 프로젝트와 관련된 레포지토리   
- [클라이언트](https://github.com/binna/VerdantValor_Client)    
  Unity 기반 게임 클라이언트입니다.  
- [서버-클라이언트 공용 코드](https://github.com/binna/VerdantValor_Shared)    
  서버와 클라이언트가 함께 사용하는 코드와 파일을 관리합니다.    
- [개발 스케줄 보드](https://github.com/users/binna/projects/1)    
  프로젝트 일정과 작업 계획을 정리한 보드입니다.    

<br><br>

## 기술스택

1. 웹 서버
 
    | 구분 | 기술 |
    |------|------|
    | **Framework** | ASP.NET Core(.NET 9.0), EF Core |
    | **DB** | MySQL, Redis |    

2. 채팅 서버

    | 구분 | 기술 |
    |------|------|
    | **Network** | TCP Socket(TcpClient/TcpListener 기반 예정) |    

<br><br>

## 부록
### A. 에코 TCP 브로드캐스트 서버 기능 테스트
1. 클라이언트 10개를 동시에 서버에 연결한 뒤,  
   서버가 수신한 메시지를 모든 클라이언트에게 브로드캐스트하여    
   정상적으로 전달되는지 확인하는 테스트입니다.
       
   [동영상보기](https://youtu.be/xZfiTMKN-EU)
       
   <details>
     <summary>이미지보기</summary>
     <img width="800" height="1400" alt="image" src="https://github.com/user-attachments/assets/46423cc8-b74b-4971-b620-87aac8450785" />
   </details>

2. 클라이언트 100개를 동시에 서버에 연결한 뒤,  
   서버가 수신한 메시지를 모든 클라이언트에게 브로드캐스트하여    
   정상적으로 전달되는지 확인하는 테스트입니다.
       
   [동영상보기](https://youtu.be/KhASnoavt8o)     

<br>

### B. 에코 TCP 브로드캐스트 서버 구조 분석
현재 구현한 에코 TCP 브로드캐스트 서버는    
각 클라이언트 세션마다 전용 `Receive` 스레드를 두는 구조입니다.    
(1 세션 = 1 스레드 = `while(true)` + `Receive`)

#### 장점
- 구현이 단순하고 직관적입니다.   
  따라서 코드 흐름이 명확하여 디버깅이나 테스트가 쉽습니다.   
- 소규모 동접(수십 ~ 많으면 수백)에서는 안정적으로 동작합니다.   

#### 단점
- 세션 수만큼 스레드가 필요합니다.    
  따라서 클라이언트가 증가할수록 스레드 수가 함께 늘어나며
  스레드 풀 고갈, 컨텍스트 스위칭 비용 증가 등의 리소스 낭비가 발생할 수 있습니다.
- 대부분의 스레드가 `Receive`에서 블로킹 대기하여   
  소켓 수가 곧 잠자는 스레드 수가 되는 구조적 문제가 발생할 수 있습니다.

<br>

### C. 에코 TCP 브로드캐스트 서버 개선 방향
#### 고려했던 대응 방안
클라이언트 연결 수 제한을 고려했습니다.   
지정한 수 이상 연결 요청은 거절하여 리소스를 보호합니다.   
안정성은 향상되지만, **근본적인 해결책은 아닙니다.**    

#### 개선 방향
이를 해결하기 위해 세션마다 스레드를 사용하는 구조 대신     
**OS에 I/O 작업을 비동기로 등록하고, 
완료 시 통지받아 이를 처리하는 방식**으로 전환할 예정입니다.    

> Windows 환경의 IOCP도 이러한 원리로 동작합니다.   

이 방식은 불필요한 스레드 생성을 줄이고,    
대규모 동접 환경에서도 안정적으로 대응할 수 있습니다.    

<br>

### D. 개선 후 테스트 및 정리

AcceptAsync를 여러 개 동시에 등록하여,   
논리적으로 여러 accept 작업을 병렬로 대기시키는 구조를 구성했습니다.    

이를 통해 스레드를 늘리지 않고도  
IOCP 스타일의 이벤트 기반 I/O 처리 흐름을 최대한 반영하고자 했습니다.    

<br>

해당 구조를 적용한 이후,    
RTT와 초당 메시지 처리량에 대한 성능 테스트를 진행했습니다.  

<br>

클라이언트 요청을 1000회 연속으로 전송하여 응답 시간을 측정했으며,    
동일한 클라이언트 실행 파일을 3회 실행하여 테스트를 반복했습니다.    

각 실행에서 수집된 응답 시간 데이터 중    
초기 구간과 종료 구간을 제외한 중앙 구간의 1000개 샘플을 기준으로   
평균 값을 산출했습니다.   

#### 테스트 결과

1. **1 세션 1 스레드 구조**    
   RTT가 수백 ms에서 1초 이상까지 반복적으로 증가하는 현상이 관찰되었습니다.    
   평균 응답 시간과 별개로 테일 레이턴시가 크게 증가하여,    
   응답 시간 분포가 불안정한 특성을 보였습니다.    
   
   <details>
     <summary>테스트 결과</summary>
     
     - 평균 RTT: 438.016786 ms
     - RTT 분포
       - 0~5ms 구간도 일부 존재
       - 500ms / 1000ms 지연이 매우 빈번하게 발생  
     - [Rtt_ThreadPerSession.txt 파일](./Echo.LoadTester/Results)
     - [동영상보기](https://youtu.be/iwqOOLmP0ks)
      
   </details>

2. **IOCP 스타일 구조**    
   RTT가 대부분 수 ms 이내로 안정적으로 유지되었으며,    
   동접 환경에서도 큰 지연 스파이크 없이    
   일관된 응답 시간 분포를 확인할 수 있었습니다.    
   
   <details>
     <summary>테스트 결과</summary>
     
     - 평균: 2.589244 ms
     - 로그 분포
       - 대부분 0.5 ~ 3ms 구간
       - 일부 요청에서 4 ~ 8ms 지연 발생
     - [Rtt_Iocp.txt 파일](./Echo.LoadTester/Results)
     - [동영상보기](https://youtu.be/PLIAcw_uxQo)
      
   </details>

#### 정리

두 구조 간 평균 RTT는 약 170배 이상의 차이를 보였으며,    

특히 1 세션 1 스레드 구조에서는    
평균과 별개로 테일 레이턴시가 크게 증가하는 특성이 두드러졌습니다.
